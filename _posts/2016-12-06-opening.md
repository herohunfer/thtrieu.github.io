---
title: A quick and relevant tutorial in Deep Learning
---

* #table of content
{:toc}

## What is it and why

I am writing this as a joint introduction to both `tensorflow` and `deep learning` for some of my interested friends. But then I realized that there might be other people like them who have sufficient background but not the time to study through texts and papers and catch up with all the hype around AI nowsaday. So it's good to just open-source the writtings and ready to learn new things, since I'm not an expert anyway.

## Who is the target reader

Those who are familiar with linear regression, logistic regression, SVM, Multilayer perceptron (MLP), overfitting, gradient descent, very basic probability and statistics. Ideally someone who fresh out of Andrew Ng's [Coursera Machine Learning](https://www.coursera.org/learn/machine-learning).

## What is in the detail?

This series of posts will cover both implementation and theoretical aspects to a reasonable extent of some `deep learning` algorithms - it is not meant to be exhautive but my promise on being relevant. Scan through the content below to see if you need it:

1. [Intro to `tensorflow`: a computational graph builder, its internal working, how to build one and solution to a non-deep-learning optimization problem.](https://thtrieu.github.io/notes/intro-to-tensorflow-and-its-first-problem)

2. A first example: MNIST digit classification with two fully connected layers, dropout, ReLU and Adaptive Momentum optimizer.

3. A gentle explanation to part 2: vanishing gradient, regularization and the need for adaptive training algorithms.

4. First basic op: convolution, convolution with 6D kernels, deconvolution and inception module.

5. Second basic op: Long short term memory, Gated Recurrent Unit, and a side kick for Neural Turing Machine.

6. Stacking layers 1: LSTM on top of Conv for sentence classification, and a side kick for Support Vector Machine.

7. Stacking layers 2: You Only Look Once: a deep convolutional net who performs detection and classification jointly by regression.

8. First assistant for training deep net: Batch normalization

9. Two other assistants for training deep net: Residual and Highway nets.

10. Attention interface: a walk through `tensorflow`'s Show and Tell code.

11. Generative models: Deep Convolutional Generative Adversarial Net, a walk through Taehoon Kim's implementation. 

## An accompany writing

There will be an accompanying series about gradient flow, exclusively about the backpropagation and relevant issues in the context of Deep Learning. I regard gradient-flow as the most relevant and insightful topic in `deep learning`, but unfortunately made easy by tools like `tensorflow`.

The discussion includes implemetation in `numpy` as well as any relevant theoretical details. The topics will include:

- [back-propagation and gradflow through `matmul` and `dropout`](https://thtrieu.github.io/notes/gradflow-debut-golden-rule-matmul-and-conv2d)

- gradflow through `conv2d` and `batch_norm`

- gradflow through `lstm` and `gru` (long short term memory and gated recurrent)

- gradflow through `loss` layer: put it all together.

Links will be filled in this post as indexes to the writtings. 
