---
title: A quick, accessible and relevant course in Deep Learning
---

* #table of content
{:toc}

## What is it and why

I am writing this as a joint introduction to both `tensorflow` and `deep learning` for some of my interested friends. But then I realized that there might be other people like them who have sufficient background but not the time to study through texts and papers and catch up with all the hype around AI nowsaday. So it's good to just open-source the writtings and ready to learn new things.

## Who is the target reader?

Those who are familiar with popular concepts of Machine Learning such as overfitting, neural ne

## What is the detail?

This series of posts will cover both implementation and theoretical aspects to a reasonable extent of some `deep learning` algorithms - it is not meant to be exhautive but my promise on being relevant and accessible. Scan through the content below to see if you need it:

1. Intro to `tensorflow`: a computational graph builder, its internal working, how to build one and solution to a non-deep-learning optimization problem.

2. A first example: MNIST digit classification with two fully connected layers, dropout, ReLU and Adaptive Momentum optimizer.

3. A gentle explanation to part 2: vanishing gradient, regularization and the need for adaptive training algorithms.

4. First basic op: convolution, convolution with 6D kernels, deconvolution and inception module.

5. Second basic op: Long short term memory, Gated Recurrent Unit, and a side kick for Neural Turing Machine.

6. Stacking layers 1: LSTM on top of Conv for sentence classification, and a side kick for Support Vector Machine.

7. Stacking layers 2: You Only Look Once: a deep convolutional net who performs detection and classification jointly by regression.

8. First assistant for training deep net: Batch normalization

9. Two other assistants for training deep net: Residual and Highway nets.

10. Attention interface: a walk through `tensorflow`'s Show and Tell code.

11. Generative models: Deep Convolutional Generative Adversarial Net, a walk through Taehoon Kim's implementation. 

## An accompany writing

There will be an accompanying series about gradient flow, exclusively about the backpropagation and relevant issues in the context of Deep Learning. I regard gradient-flow as the most relevant and insightful topic in `deep learning`, but unfortunately made easy by tools like `tensorflow` and therefore, is ignored most of the time by practitioners. The discussion includes implemetation as well as any relevant theoretical details. The topics will include:

- gradflow through `matmul` and `conv2d` (fully connected and 2D convolutional)

- gradflow through `lstm` and `gru` (long short term memory and gated recurrent)

- gradflow through batch normalization

Links will be filled in this post as indexes to the writtings. 
